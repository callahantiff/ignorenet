---
title: "BioHackathon 2017"
output:
  html_notebook: default
---

---

#### Contents:

* [Introduction](#introduction)
* [Accessing GEO Data: No Raw Data Availible](#data1)
    * [Create Expression Dataset](#eset1)
    * [Data Pre-Processing](#preprc1)
* [Accessing GEO Data: Raw Data Available](#data2)
    * [Affymetrix](#affy)
    * [Illumina](#illum)
    * [Agilent](#agil)
    * [Applied Biosystems](#abarray)
* [Differential Expression Analysis](#dea)
* [Retrieving Metadata](#metadata)
* [Workflow Reproduciblity](#repo)



---
    
<a name="introduction"/> 

## Introduction
This script is designed to download and process data from [GEO](https://www.ncbi.nlm.nih.gov/geo/). For each downloaded dataset, the data is processed and converted into an expression matrix (rows are genes and columns are samples). Additionally, using the [GEO Metadata Database](https://www.ncbi.nlm.nih.gov/geo/info/geo_paccess.html) specific features for each dataset are collected and kept for downstream processing and analysis. Prior to running differential expression analysis, the expression data is verified to ensure both normalization and log transformation had been performed. Differential expression analysis is performed using limma and leverages code from existing workflows ([Law et al., 2014](https://www.ncbi.nlm.nih.gov/pubmed/24485249), [limma user guide](http://bioconductor.org/packages/release/bioc/vignettes/limma/inst/doc/usersguide.pdf)).


```{r message=FALSE, warning=FALSE}
# load needed libraries
library(annotate)
library(Biobase)
library(data.table)
library(EMA)
library(GEOmetadb)
library(GEOquery)
library(ggplot2)
library(gplots)
library(knitr)
library(latex2exp)
library(limma)
library(mygene)
library(RColorBrewer)
library(R.utils)
```

<a name="studs"/>  
## Query GEO for Studies Meeting Specific User Criteria
```{r warning=FALSE, message=FALSE, eval=FALSE, cache=FALSE}
# connect to database
con <- DBI::dbConnect(SQLite(),'GEO_Database/GEOmetadb.sqlite')

# query database
metadata <- DBI::dbGetQuery(con,'SELECT gse, pubmed_id, title, summary, submission_date, type, overall_design
FROM gse
WHERE gse IN (
"GSE4707",
"GSE6573",
"GSE10588",
"GSE25906",
"GSE14722",
"GSE24129",
"GSE30186",
"GSE43942",
"GSE47187",
"GSE74341",
"GSE35574",
"GSE44711",
"GSE60438",
"GSE73374")')

# print results
knitr::kable(metadata)

# replace tabs in strings with pipes
metadata$title = base::gsub("\t", " | ", metadata$title)
metadata$summary = base::gsub("\t", " | ", metadata$summary)
metadata$type = base::gsub("\t", " | ", metadata$type)
metadata$overall_design = base::gsub("\t", " | ", metadata$overall_design)

# write metadata to csv
utils::write.table(metadata, 
            'GEO_Data/GES_Metadata.txt', 
            row.names = FALSE, 
            quote = FALSE,
            sep = '\t')

# close db connection
DBI::dbDisconnect(con)


```


<a name="data1"/>   

## Accessing GEO Data: No Raw Data Availible
When there is no raw data available on GEO, the GEO Data Set (GDS) SOFT files are downloaded and processed. If the GSE Accession identifier does not have a corresponding GSD identifier, the GSE matrix can be downloaded and the expression data and annotation file it contains can be re-analyzed. In these scenarios, the data are analyzed with caution and all pre-processing steps described by the authors are verified in an attempt to make the analysis pipeline across platforms as consistent as possible.
```{r warning=FALSE, message=FALSE, eval=FALSE, cache=FALSE}
## Download GDS
# set GSE identifier
id <- 'GDS2080' #GSE4707

# download GDS file
url <- paste('ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/',
             stringr::str_sub(id, 1,4),
             'nnn/', 
             id,
             '/soft/',
             id,
             '.soft.gz', 
             sep = '')

# create directory to download data to
dir.create(paste('Raw_GEO_Data/',id, sep=''))

# download data to directory
utils::download.file(url, destfile=paste('Raw_GEO_Data/', id, '/', id, '.soft.gz', sep=''), mode='wb')

# open GDS
gds <- GEOquery::getGEO(filename=paste('Raw_GEO_Data/', id, '/', id, '.soft.gz', sep=''))


## GSE Data
# set the GSE identifier
id = 'GSE10588'

# use the getGEO() function to download the GEO data for the id
base::assign(paste(id, '_gse_data', sep = ''), 
       GEOquery::getGEO(id, GSEMatrix=TRUE, AnnotGPL=FALSE))
gse_data <- GSE10588_gse_data

# create gene expression set. If multiple data sets, take the first one and retrieve platform-specific annotation information
if (length(gse_data) > 1) idx <- grep(gse_data[[1]]@annotation, attr(gse_data, 'names')) else idx <- 1
```

<a name="eset1"/> 

### Create Expression Set
Create an expression set from the extracted data extracted from GEO. Additionally, this step also  accesses sample group assignment information from the downloaded GSE matrix.
```{r warning=FALSE, message=FALSE, eval=FALSE, cache=FALSE}
## GDS
# print column names
colnames(GEOquery::Table(gds))

# get experimental data
eset <- GDS2eSet(gds, do.log2=FALSE)
exprs <- Biobase::exprs(eset)

# get gene annotation information
fset <- Biobase::fData(eset)
genes = fset[,c(1, 4, 3)]
names(genes) = c('Genes.identifier', 'Genes.ENTREZID', 'Genes.SYMBOL')


## GSE
# view dataset information
gse_exp_set <- gse_data[[idx]]
gse_exp_set

# update column names
Biobase::fvarLabels(gse_exp_set) <- make.names(Biobase::fvarLabels(gse_exp_set))

# convert expression set to expression matrix
exprs_data <- Biobase::exprs(gse_exp_set)

# store gene names and samples
genes = data.frame(rownames(exprs_data), 
                       gse_data[[1]]@featureData@data$GeneID,
                       gse_data[[1]]@featureData@data$`Gene Symbol`)

# name columns
names(genes) = c('probe', 'Genes.ENTREZID', 'Genes.SYMBOL')
samples = colnames(exprs_data)

# set group membership for all samples
# GSE43942
groups = as.character(lapply(as.character(gse_exp_set@phenoData@data$source_name_ch1), function(x) ifelse(length(grep('norm', x)) == 1, 'control', 'PE')))

# GSE4707
# get sample grouping information
groups <- as.data.frame(Biobase::pData(eset)[-1][1], stringsAsFactors=FALSE)
groups <- as.character(groups[,1])

# rename
groups = as.character(lapply(groups, function(x) ifelse(length(grep('early', x)) == 1, 'EO', x)))
groups = as.character(lapply(groups, function(x) ifelse(length(grep('late', x)) == 1, 'LO', x)))
groups = as.character(lapply(groups, function(x) ifelse(length(grep('normal', x)) == 1, 'Control', x)))

# print groups
table(groups)
```


<a name="preproc1"/>  

### Pre-Processing and Quality Control
Prior to determining which genes are differentially expressed, the preprocessing performed by the submitter of the data is verified. Specifically, the data is checked to ensure normalization and log transformation. Additional verification performed on the data includes checking for 'NA' values, removing duplicate genes, filtering out lowly expressed genes, and verifying sample clustering.
```{r cache=FALSE, echo=TRUE, vmessage=FALSE, warning=FALSE, eval=TRUE}
## log Transformation
# preview data - verify that the data has been log2 transformed
avg <- base::rowMeans(exprs)
summary(avg)
exprs_data <- exprs

# if aglient, then data needs to be converted back to log2 from log10 (GSE4707)
exprs_data <- base::log(10**exprs, 2)

## Checking for NAs
cat('There are', base::sum(!complete.cases(exprs_data)) , 'rows with NA values')
exprs_data_cut = exprs_data[complete.cases(exprs_data), ]

## Normalization
# create box plots to verify pre-processing
boxplot(exprs_data_cut, col = brewer.pal(9, 'Set1'), 
        cex.axis = 0.5, cex.lab = 0.65, las=2,
        ylab = TeX('$log_{2}$ Intensity'))

# remove duplicate gene names
cat('There are', dim(exprs_data_cut)[1], 'genes before removing duplicates')
exprs_data_clean = exprs_data_cut[!duplicated(rownames(exprs_data_cut)),]
cat('There are', dim(exprs_data_clean)[1], 'genes after removing', dim(exprs_data_cut)[1]-dim(exprs_data_clean)[1], 'duplicates')

## Filtering
# summarize average gene expression values
avg <- rowMeans(exprs_data_clean)
sum = summary(avg)

# filter out genes that "aren't expressed" - will appear red in the plot
cutoff <- sum[2][[1]] # remove genes below first quantile
exprs_data_clean = expFilter(exprs_data_clean, 
                     threshold = round(sum[2][[1]], 4))

# # remove genes with expression below first quantile
cat('There are', dim(exprs_data_clean)[1], 'genes after removing', dim(exprs_data_cut)[1]-dim(exprs_data_clean)[1], 'when filtering out genes with expresison values below the first quartile')
```

```{r cache=FALSE, echo=TRUE, vmessage=FALSE, warning=FALSE, eval=TRUE}
## Clustering
# cluster expression data by sample
sample = EMA::clustering(data = exprs_data_clean, metric = 'pearson', method = 'average')
EMA::clustering.plot(tree = sample, lab = groups, title = 'Filtered Data', scale='row', legend.pos = 'topleft')
```

```{r cache=FALSE, echo=TRUE, vmessage=FALSE, warning=FALSE, eval=TRUE}
## Principle Components Analysis
# segment variation into different components
acp = EMA::runPCA(t(exprs_data_clean), scale = FALSE, lab.sample = groups, plotSample = FALSE, plotInertia = FALSE)
names(acp$eig) = c('eigen', '%var', 'cumm %var')

# print results
knitr::kable(head(acp$eig))

# view variation between samples
EMA::plotSample(acp, axes = c(1, 2), lab = groups)

# save cleaned expression data
exp = as.data.frame(cbind(rownames(exprs_data_clean), exprs_data_clean))
names(exp) = c('probe', colnames(exprs_data_clean))
write.table(exp, file = paste('GEO_Data/', id, '_clean_exp_data.txt', sep = ''), 
            sep = '\t',
            row.names = FALSE,
            col.names = TRUE,
            quote = FALSE)
```
 


<a name="data2"/>   

## Accessing GEO Data: Raw Data Availible
When there is raw data available on GEO, the supplementary data for each study are downloaded and processed. Specifically, the non-normalized data is downloaded. Raw data for each study are stored in a directory named after the study's GSE Accession identifier.
```{r warning=FALSE, message=FALSE, eval=FALSE, cache=FALSE}
## Download Data
id <- 'GSE74341'
url <- paste('https://www.ncbi.nlm.nih.gov/geo/download/?acc=', id, '&format=file', sep = '')
utils::download.file(url, destfile=paste('Raw_GEO_Data/', id, '_RAW.tar', sep=''), mode='wb')
  
# unzip data to directory, where directory has same name as id
utils::untar(paste('Raw_GEO_Data/', id, '_RAW.tar', sep=''), exdir = base::paste('Raw_GEO_Data/', id, sep=''))

# delete zipped file
base::file.remove(paste('Raw_GEO_Data/', id, '_RAW.tar', sep=''))
```

### Get Target Information  
As most of the studies on GEO do not contain files containing target information, this information must be extracted from each study's Geo Study Matrix. The code below is designed to specifically extract sample label information that can be used to create the groupings for differential expression analysis.
```{r warning=FALSE, message=FALSE, eval=FALSE, cache=FALSE}
# download GSE study data
gse_data <- GEOquery::getGEO(id, GSEMatrix=TRUE, AnnotGPL=FALSE)

# take the first and retrieve platform-specific annotation information
if (length(gse_data) > 1) idx <- grep(gse_data[[1]]@annotation, attr(gse_data, 'names')) else idx <- 1

# view dataset information
gse_exp_set <- gse_data[[idx]]
gse_exp_set

# GSE6573, GSE14722, GSE73374, GSE30186
groups = as.character(lapply(as.character(gse_exp_set@phenoData@data$title), function(x) ifelse(length(grep('CON', x)) == 1, 'control', 'PE')))

#GSE44711
groups = as.character(lapply(as.character(gse_exp_set@phenoData@data$title), function(x) ifelse(length(grep('eclamp', x)) == 1, 'PE', 'control')))

#GSE24129
groups = as.character(lapply(as.character(gse_exp_set@phenoData@data$title), function(x) ifelse(length(grep('Norm', x)) == 1, 'control', x)))
groups = as.character(lapply(groups, function(x) ifelse(length(grep('Fetal', x)) == 1, 'FGR', x)))
groups = as.character(lapply(groups, function(x) ifelse(length(grep('Pre-', x)) == 1, 'PE', x)))

# GSE25906
groups = as.character(lapply(as.character(gse_exp_set@phenoData@data$characteristics_ch1.2), function(x) ifelse(length(grep('preeclamp', x)) == 1, 'PE', 'control')))

# GSE35574
groups = as.character(lapply(as.character(gse_exp_set@phenoData@data$characteristics_ch1.2), function(x) ifelse(length(grep('CON', x)) == 1, 'control', x)))
groups = as.character(lapply(groups, function(x) ifelse(length(grep('IUG', x)) == 1, 'IUGR', x)))
groups = as.character(lapply(groups, function(x) ifelse(length(grep('PE', x)) == 1, 'PE', x)))

# GSE60438
groups = as.character(lapply(as.character(gse_exp_set@phenoData@data$source_name_ch1), function(x) ifelse(length(grep('norm', x)) == 1, 'control', 'PE')))

# create groups
# GSE47187
groups = base::rep(c('control', 'PE'), 5)

#GSE74341
groups = c('control', 'EOPE', 'control', 'EOPE', 'control', 'EOPE', 'control', 'EOPE', 'control', 'EOPE', 'control', 'EOPE', 'control', 'EOPE', 'control', 'LOPE', 'control', 'LOPE', 'control', 'LOPE', 'control', 'LOPE', 'control', 'LOPE', 'control', 'LOPE', 'control', 'LOPE', 'control', 'LOPE', 'control', 'term', 'control', 'term', 'control', 'term', 'control', 'term', 'control', 'term', 'control', 'preterm', 'control', 'preterm', 'control', 'preterm', 'control', 'preterm', 'control', 'preterm')

# GSE10588
groups = as.character(lapply(as.character(gse_exp_set@phenoData@data$title), function(x) ifelse(length(grep('preeclampsia', x)) == 1, 'SPE', 'control')))

# print groups
table(groups)
```


<a name="affy"/> 

### Process Affymetrix Data
Downloaded CEL files for each study utilizing Affymetrix are stored in folders and named after the study GSE Accession ID. The CEL files within each study-specific directory are converted to an AffyBatch object and normalized, background corrected, and $log_2$ transformed using Affy or Oligo RMA. Filtering was also performed and all genes with expression levels below the first quartile were removed. Probes are re-annotated using current annotation files from Bioconductor. The general workflow for processing Affymetrix data described in the [limma User Guide](http://www.bioconductor.org/packages/release/bioc/vignettes/limma/inst/doc/usersguide.pdf) was used.
```{r cache=FALSE, echo=TRUE, vmessage=FALSE, warning=FALSE, eval=TRUE}
## Load Libraries
library(affy)
library(affycoretools)
library(oligo)
library(hgu133plus2.db) #GSE6573
library(hgu133a.db) #GSE14722
library(hgu133b.db) #GSE14722
library(hugene10sttranscriptcluster.db) #GSE24129
library(hugene20sttranscriptcluster.db) # GSE73374

# import CEL files + read into AffyBatch
CEL.files <- base::list.files(path = paste('Raw_GEO_Data/', id, sep=''), full.names=TRUE)
affy.data = affy::ReadAffy(filenames = CEL.files)

# for Affymetrix Human Gene 2.0 ST Array
affy.data = oligo::read.celfiles(CEL.files)

## normalize the data
affy.norm = affy::rma(affy.data)
affy.norm.exprs = Biobase::exprs(affy.norm)

# normalized intensity values boxplot
graphics::boxplot(affy.norm.exprs, col = brewer.pal(9, 'Set1'), 
        cex.axis = 0.5, cex.lab = 0.65, las=2,
        ylab = TeX('$log_{2}$ Intensity'))

## Filtering
# summarize average gene expression values
avg <- rowMeans(affy.norm.exprs)
sum = summary(avg)

# filter out genes that "aren't expressed" - will appear red in the plot
cutoff <- sum[2][[1]] # remove genes below first quantile
exp_filt = expFilter(affy.norm.exprs, threshold = round(sum[2][[1]], 4))

# remove genes below first quartile
cat('There are', dim(exp_filt)[1], 'genes after removing', dim(affy.norm.exprs)[1]-dim(exp_filt)[1], 'when filtering out genes with expresison values below the first quartile')

# cluster expression data by sample
sample = EMA::clustering(data = exp_filt, metric = 'pearson', method = 'average')
EMA::clustering.plot(tree = sample, lab = groups, title = 'Filtered Data', scale='row', legend.pos = 'topleft')

## Principle Components Analysis
# segment variation into different components
acp = EMA::runPCA(t(exp_filt), scale = FALSE, lab.sample = groups, plotSample = FALSE, plotInertia = FALSE)
names(acp$eig) = c('eigen', '%var', 'cumm %var')

# print results
knitr::kable(head(acp$eig))

# view variation between samples
EMA::plotSample(acp, axes = c(1, 2), lab = groups)

# save cleaned expression data
exp = as.data.frame(cbind(rownames(exp_filt), exp_filt))
names(exp) = c('probe', colnames(exp_filt))
write.table(exp, file = paste('GEO_Data/', id, '_clean_exp_data.txt', sep = ''), 
            sep = '\t',
            row.names = FALSE,
            col.names = TRUE,
            quote = FALSE)

# rename saved file
exprs_data_clean = exp_filt

## Annotate Probes
# print columns
columns(hugene20sttranscriptcluster.db)

# annotate genes
genes_annot <- AnnotationDbi::select(hgu133plus2.db, 
                               keys=rownames(exprs_data_clean),
                               columns=c('SYMBOL', 'ENTREZID'), 
                               keytype='PROBEID')

# GSE73374
genes_a <- affycoretools::annotateEset(oligo::rma(affy.data, target="core"), pd.hugene.2.0.st)

genes_annot <- AnnotationDbi::select(hugene20sttranscriptcluster.db, 
                            keys=as.character(genes_a@featureData@data$PROBEID),
                               columns=c('SYMBOL', 'ENTREZID'), 
                               keytype='PROBEID')

# aggregate 1:many mappings into single list
entrez = aggregate(ENTREZID ~ PROBEID, 
                   data = genes_annot, 
                   paste, collapse = ' /// ')

symbol = aggregate(SYMBOL ~ PROBEID, 
                   data = genes_annot, 
                   paste, collapse = ' /// ')

# merge lists together
genes = merge(entrez, symbol, by = 'PROBEID', all = TRUE)
```


<a name="illum"/> 

### Process Illumina Data
Downloaded non-normalized text files for each study utilizing Illumina are stored in folders and named using the study GSE Accession ID. Each text file contains a probe summary profile. Processing Illumina data requires a few extra steps, specifically, we must know which files represent targets and which represent controls prior to reading in the data. The text files within each study-specific directory are converted to an Elist object and quartile normalized, background corrected using negative controls, and $log_2$ transformed. Filtering was also performed and all genes with expression levels below the first quartile are removed.  Probes are re-annotated using current annotation files from Bioconductor. The general workflow for processing BeadArray data described in the [limma User Guide](http://www.bioconductor.org/packages/release/bioc/vignettes/limma/inst/doc/usersguide.pdf) was used.
```{r cache=FALSE, echo=TRUE, vmessage=FALSE, warning=FALSE, eval=TRUE}
## Load Libraries
library(illuminaHumanv2.db)
library(illuminaHumanv4.db)

# import text files
txt.files <- base::list.files(path = paste('Raw_GEO_Data/', id, sep=''), full.names=TRUE)

# read in probes to access probe names
probes = utils::read.table(txt.files, sep = '\t', header = TRUE)

# read in non-normalized data
illum.data = limma::read.ilmn(txt.files)

## Background Correction
illum.data.bkg  <- limma::backgroundCorrect(illum.data, method="normexp", normexp.method = 'rma')

# create expression data set
illum.bkg.exprs <- illum.data.bkg@.Data[[2]]
rownames(illum.bkg.exprs) <- as.character(probes[,1])

## Normalization
illum.bkg.exprs.norm  <- limma::normalizeQuantiles(illum.bkg.exprs, ties=TRUE)

## Log2 Transformation
illum.bkg.exprs.norm.lg <- base::log(illum.bkg.exprs.norm, 2)

# normalized intensity values boxplot
graphics::boxplot(illum.bkg.exprs.norm.lg, col = brewer.pal(9, 'Set1'), 
        cex.axis = 0.5, cex.lab = 0.65, las=2,
        ylab = TeX('$log_{2}$ Intensity'))

## Filtering
# summarize average gene expression values
avg <- rowMeans(illum.bkg.exprs.norm.lg)
sum = summary(avg)

# filter out genes that "aren't expressed" - will appear red in the plot
cutoff <- sum[2][[1]] # remove genes below first quantile
exp_filt = expFilter(illum.bkg.exprs.norm.lg, threshold = round(sum[2][[1]], 4))

# remove genes below first quantile
cat('There are', dim(exp_filt)[1], 'genes after removing', dim(affy.norm.exprs)[1]-dim(exp_filt)[1], 'when filtering out genes with expresison values below the first quartile')

# cluster expression data by sample
sample = EMA::clustering(data = exp_filt, metric = 'pearson', method = 'average')
EMA::clustering.plot(tree = sample, lab = groups, title = 'Filtered Data', scale='row', legend.pos = 'topleft')

## Principle Components Analysis
# segment variation into different components
acp = EMA::runPCA(t(exp_filt), scale = FALSE, lab.sample = groups, plotSample = FALSE, plotInertia = FALSE)
names(acp$eig) = c('eigen', '%var', 'cumm %var')

# print results
knitr::kable(head(acp$eig))

# view variation between samples
EMA::plotSample(acp, axes = c(1, 2), lab = groups)

# save cleaned expression data
exp = as.data.frame(cbind(rownames(exp_filt), exp_filt))
names(exp) = c('probe', colnames(exp_filt))
write.table(exp, file = paste('GEO_Data/', id, '_clean_exp_data.txt', sep = ''), 
            sep = '\t',
            row.names = FALSE,
            col.names = TRUE,
            quote = FALSE)

# rename saved file
exprs_data_clean = exp_filt

## Annotate Probes
# print columns
columns(illuminaHumanv2.db)

# annotate genes
genes_annot <- AnnotationDbi::select(illuminaHumanv3.db, 
                               keys=rownames(exprs_data_clean),
                               columns=c('SYMBOL', 'ENTREZID'), 
                               keytype='PROBEID')

# GSE73374
genes_a <- affycoretools::annotateEset(oligo::rma(affy.data, target="core"), pd.hugene.2.0.st)

genes_annot <- AnnotationDbi::select(hugene20sttranscriptcluster.db, 
                            keys=as.character(genes_a@featureData@data$PROBEID),
                               columns=c('SYMBOL', 'ENTREZID'), 
                               keytype='PROBEID')

# aggregate 1:many mappings into single list
entrez = aggregate(ENTREZID ~ PROBEID, 
                   data = genes_annot, 
                   paste, collapse = ' /// ')

symbol = aggregate(SYMBOL ~ PROBEID, 
                   data = genes_annot, 
                   paste, collapse = ' /// ')

# merge lists together
genes = merge(entrez, symbol, by = 'PROBEID', all = TRUE)
```


<a name="agil"/> 

### Process Agilent Data
Downloaded raw text files for each study utilizing Agilent are stored in folders and named after the study GSE Accession ID. In addition to the data files, a targets text file is included. The text files within each study-specific directory are converted to an RGList object and are loess normalized, background corrected, and $log_2$ transformed. Filtering was also performed and all genes with expression levels below the first quartile are removed.  Probes are re-annotated using current annotation files from Bioconductor. The general workflow for processing BeadArray data described in the [limma User Guide](http://matticklab.com/index.php?title=Two_channel_analysis_of_Agilent_microarray_data_with_Limma) and the [Mattick Lab Wiki](http://matticklab.com/index.php?title=Two_channel_analysis_of_Agilent_microarray_data_with_Limma).
```{r cache=FALSE, echo=TRUE, vmessage=FALSE, warning=FALSE, eval=TRUE}
# import targets
targets <- limma::readTargets(paste('Raw_GEO_Data/', id, '/targets.txt', sep=''))
targets

# load in data as an RG object
RG <- limma::read.maimages(targets, 
                           path=paste('Raw_GEO_Data/', id, sep=''),
                           source="agilent.median")

## Background Correction
agil.data.bkg <- limma::backgroundCorrect(RG, method="normexp", normexp.method = 'rma')

## Normalization
agil.bkg.norm <- limma::normalizeWithinArrays(agil.data.bkg, method="loess")

# average replicate spots
agil.bkg.norm.avg <- avereps(agil.bkg.norm, 
                              ID=agil.bkg.norm$genes$ProbeName)

# set probes
probes = agil.bkg.norm.avg$genes$ProbeName

# extract log transformed expression matrix - green/red (aka control/PE)
agil.bkg.norm.avg.lg.exprs = limma::exprs.MA(agil.bkg.norm.avg)

# label expression matrix rows
rownames(agil.bkg.norm.avg.lg.exprs) <- probes

# remove probes that are controls
agil.bkg.norm.avg.lg.exprs <- agil.bkg.norm.avg.lg.exprs[ rownames(agil.bkg.norm.avg.lg.exprs) %in% fset$ID, ]

# normalized intensity values boxplot
graphics::boxplot(agil.bkg.norm.avg.lg.exprs, col = brewer.pal(9, 'Set1'), 
        cex.axis = 0.5, cex.lab = 0.65, las=2,
        ylab = TeX('$log_{2}$ Intensity'))

## Filtering
# summarize average gene expression values
avg <- rowMeans(agil.bkg.norm.avg.lg.exprs)
sum = summary(avg)

# filter out genes that "aren't expressed" - will appear red in the plot
cutoff <- sum[2][[1]] # remove genes below first quantile
exp_filt = expFilter(agil.bkg.norm.avg.lg.exprs, 
                     threshold = round(sum[2][[1]], 4))

# remove genes below first quantile
cat('There are', dim(exp_filt)[1], 'genes after removing', dim(agil.bkg.norm.avg.lg.exprs)[1]-dim(exp_filt)[1], 'when filtering out genes with expresison values below the first quartile')

# cluster expression data by sample
sample = EMA::clustering(data = exp_filt, metric = 'pearson', method = 'average')
EMA::clustering.plot(tree = sample, lab = groups, title = 'Filtered Data', scale='row', legend.pos = 'topleft')

## Principle Components Analysis
# segment variation into different components
acp = EMA::runPCA(t(exp_filt), scale = FALSE, lab.sample = groups, plotSample = FALSE, plotInertia = FALSE)
names(acp$eig) = c('eigen', '%var', 'cumm %var')

# print results
knitr::kable(head(acp$eig))

# view variation between samples
EMA::plotSample(acp, axes = c(1, 2), lab = groups)

# save cleaned expression data
exp = as.data.frame(cbind(rownames(exp_filt), exp_filt))
names(exp) = c('probe', colnames(exp_filt))
write.table(exp, file = paste('GEO_Data/', id, '_clean_exp_data.txt', sep = ''), 
            sep = '\t',
            row.names = FALSE,
            col.names = TRUE,
            quote = FALSE)

# rename saved file
exprs_data_clean = exp_filt

## Annotate Probes
# read in annotation file downloaded from Agilent (9/4/2017)
agilent_annotations.db <- read.table('RAW_GEO_Data/014850_D_AA_20070207.txt',
                                     header = TRUE,
                                     stringsAsFactors = FALSE,
                                     fill = TRUE,
                                     sep = "\t")
# print columns
names(agilent_annotations.db)

# aggregate 1:many mappings into single list
entrez = aggregate(EntrezGeneID ~ ProbeID, 
                   data = agilent_annotations.db, 
                   paste, collapse = ' /// ')

symbol = aggregate(GeneSymbol ~ ProbeID, 
                   data = agilent_annotations.db, 
                   paste, collapse = ' /// ')

# merge lists together
genes = merge(entrez, symbol, by = 'ProbeID', all = TRUE)[-1,]
```


<a name="abarray"/> 

### Process Applied Biosystems Genome Survey Data
Downloaded raw text files for each study utilizing Applied Biosystems Genome Survey microarrays are stored in folders and named after the study GSE Accession ID. This workflow is still under construction. The general workflow for processing BeadArray data described in the [ABarray User Guide](https://bioconductor.riken.jp/packages/3.0/bioc/vignettes/ABarray/inst/doc/ABarray.pdf).
```{r cache=FALSE, echo=TRUE, vmessage=FALSE, warning=FALSE, eval=TRUE}
# load libraries
library(ABarray)

## create data file
# list files in a directory
file_list <- base::list.files(path = paste('Raw_GEO_Data/', id, '/raw_data', sep=''), full.names=TRUE)

# merge all files in the directory into a single data frame
for (file in file_list){

  # if the merged dataset doesn't exist, create it
  if (!base::exists("combined_ABI_data")){
    combined_ABI_data <- utils::read.table(file, header=TRUE, sep="\t")
    combined_ABI_data$file <- rep(file, nrow(combined_ABI_data))
  }
   
  # if the merged dataset does exist, append to it
  if (base::exists("combined_ABI_data")){
    temp_dataset <- utils::read.table(file, header=TRUE, sep="\t")
    temp_dataset$file <- rep(file, nrow(temp_dataset))
    combined_ABI_data <- base::rbind(combined_ABI_data, temp_dataset)
    base::rm(temp_dataset)
  }
}

# update dataframe to only include certain columns in a specific order
vars <- names(combined_ABI_data) %in% c("Gene_ID", "Probe_ID", "Signal", "Flags", "S_N")

# subset folder
up_combo_data <- combined_ABI_data[vars]

# rename folder columns
base::names(up_combo_data) <- c("geneID", "probeID", "Signal", "Flags", "S/N")

# write file to directory
utils::write.table(up_combo_data,
            paste('Raw_GEO_Data/', id, '/merged_datafile.txt', sep=''),
            row.names = FALSE, 
            quote = FALSE,
            sep = '\t')

## create the experimental design file
# subset data to only include unqiue file and assay names
exp_info <- unique(combined_ABI_data[c("file", "Assay_Name")])

# remove spaces from strings
exp_info$Assay_Name <- unique(gsub(" ", "_", exp_info$Assay_Name))

# add grouping information (only hard-coded bc working on the airplane)
tissue <- c(rep('control', 11), rep('PE', 1), rep('control', 15), rep('PE', 16))

# combine exp info into dataframe
exp_info_grp <- cbind(exp_info, tissue)
names(exp_info_grp) <- c("sampleName", "assayName", "group")

# write file to directory
utils::write.table(exp_info_grp,
            paste('Raw_GEO_Data/', id, '/merged_exp_info.txt', sep=''),
            row.names = FALSE, 
            quote = FALSE,
            sep = '\t')

## Analyze data
eset = ABarray(paste('Raw_GEO_Data/', id, '/merged_datafile.txt', sep=''), 
               paste('Raw_GEO_Data/', id, '/merged_exp_info.txt', sep=''), "group")

```

<a name="dea"/>  

### Differential Expression Analysis
Differential expression analysis was performed on the pre-processed microarray using limma ( [microarray](http://bioconductor.org/packages/release/bioc/vignettes/limma/inst/doc/usersguide.pdf)).
```{r cache=FALSE, echo=TRUE, error = FALSE, vmessage=FALSE, warning=FALSE, eval=TRUE}
# create design matrix
design <- stats::model.matrix(~ groups + 0, data.frame(exprs_data_clean))
colnames(design) <- gsub('groups', '', colnames(design))
design

# create constrast (the one we subtract is control, then we interpret results for cases)
contr.matrix <- limma::makeContrasts(
   PEvsC = PE-control,
   levels = colnames(design))
contr.matrix

# fit model
fit <- limma::lmFit(data.frame(exprs_data_clean), design)
fit <- limma::contrasts.fit(fit, contrasts=contr.matrix)
efit <- limma::eBayes(fit)

# include only those genes that still exist after pre-processing
rownames(genes) = genes[,1]
GSE_genes <- genes[rownames(exprs_data_clean),]

# add gene information to efit
efit$genes <- GSE_genes

# print number of up and down regulated genes between the samples
top_dec_test <- decideTests(efit)
knitr::kable(summary(top_dec_test))

## Process Results
limma::write.fit(efit, top_dec_test, adjust='BH', file=paste('DE_Results/', id ,'_DEgenes.txt', sep = ''))
```
 

```{r cache=FALSE, echo=TRUE, vmessage=FALSE, warning=FALSE, eval=TRUE}
## Visualize Results
# MD plot
plotMD(efit, column=1, status=top_dec_test[,1], main=colnames(efit)[1], cex = 0.7)

# volcano plot
ggplot2::theme_update(plot.title = element_text(hjust = 0.5))
ggplot2::ggplot(data = results, aes(x = results$logFC, y = -log10(results$adj.P.Val)), colour = none) + 
       geom_point(alpha = 0.4, size = 1.75) + 
       labs(title = paste(id, ': Volcano Plot', sep = ' ')) + 
       xlim(c(ceiling(-max(results$logFC)), ceiling(max(results$logFC)))) + 
       ylim(c(0, ceiling(max(-log10(results$adj.P.Val))))) +
       ylab(TeX('$-log_{10}$ P-Value')) + 
       xlab(TeX('$log_{2}$ Fold Change'))
```

```{r cache=FALSE, echo=TRUE, vmessage=FALSE, warning=FALSE, eval=TRUE}
## Heatmap - top 100 genes
mvgenes = as.character(rownames(results)[1:100])
c.sample <- EMA::clustering(data = exprs_data_clean[mvgenes, ], metric = 'pearson', method = 'ward')
c.gene <- EMA::clustering(data = t(exprs_data_clean[mvgenes, ]), metric = 'pearson', method = 'ward')
EMA::clustering.plot(tree = c.sample, tree.sup = c.gene, data = exprs_data_clean[mvgenes, 
    ], names.sup = FALSE, lab = groups, trim.heatmap = 0.99, scale = 'row')
```




<a name="metadata"/> 

## GEO Metadata Retrieval
The GEO Metadata Database is used to retrieve important metadata about each GSE accession ID used in the current analysis.
```{r warning=FALSE, cache=TRUE}
# connect to database
con <- DBI::dbConnect(SQLite(),'GEO_Database/GEOmetadb.sqlite')

# query database
metadata <- DBI::dbGetQuery(con,'SELECT DISTINCT organism
FROM gpl')

# print results
knitr::kable(metadata)

# replace tabs in strings with pipes
metadata$title = base::gsub("\t", " | ", metadata$title)
metadata$summary = base::gsub("\t", " | ", metadata$summary)
metadata$type = base::gsub("\t", " | ", metadata$type)
metadata$overall_design = base::gsub("\t", " | ", metadata$overall_design)

# write metadata to csv
utils::write.table(metadata, 
            'GEO_Data/GES_Metadata.txt', 
            row.names = FALSE, 
            quote = FALSE,
            sep = '\t')

# close db connection
DBI::dbDisconnect(con)
```


<a name="repo"/>  

#### Reproducibility
Print session information.
```{r eval=TRUE}
sessionInfo()
```


```{r}

# list files in a directory
file_list <- base::list.files(path = 'DE_Results/', full.names=TRUE)

# read in gene data
for (file in file_list){
  # if the merged dataset doesn't exist, create it
  if (!exists("DE_genes")){
    dataset <- read.table(file, header=TRUE)
  }
   
  # if the merged dataset does exist, append to it
  if (exists("DE_genes")){
    temp_dataset <-read.table(file, header=TRUE)
    dataset<-rbind(dataset, temp_dataset)
    rm(temp_dataset)
  }
 
}

genes12= read.table("DE_Results/GSE74341_DEgenes.txt", header = TRUE, "\t")

genes = unique(genes7$Genes.SYMBOL)

write.table(genes, 'PE_genes.txt', row.names = FALSE, col.names = FALSE)


endpoint = 'http://amc-tantor.ucdenver.pvt:10035/repositories/kabob_prod_sep2017'


# create query statement
query <- "
prefix franzOption_chunkProcessingAllowed: <franz:yes>
prefix franzOption_clauseReorderer: <franz:identity>
PREFIX franzOption_memoryLimit: <franz:60g>
PREFIX franzOption_memoryExhaustionWarningPercentage: <franz:90.0>

PREFIX obo_pr: <http://purl.obolibrary.org/obo/pr#>
PREFIX ccp: <http://ccp.ucdenver.edu/obo/ext/>
PREFIX obo: <http://purl.obolibrary.org/obo/>

# Drug interacts with protein encoded by gene with association to disease/phenotype.

select * {
  # Start by finding the 'drug' class in bioworld, 
  # then get all potential drugs within bioworld (subclasses of the 'drug' class).
  ccp:CHEBI_23888 obo:IAO_0000219 ?drug_bioentity . # CHEBI_23888 = CHEBI:drug; IAO_0000219 = IAO:denotes
  filter (?drug_bioentity != obo:CHEBI_23888) # CHEBI_23888 = CHEBI:drug
  ?drug rdfs:subClassOf ?drug_bioentity .
  ?drug_ice obo:IAO_0000219 ?drug .
  
  # Find drugs that are involved in has_participant restrictions 
  # as the object of owl:someValuesFrom relations.
  ?drug_sc rdfs:subClassOf ?drug .
  ?drug_participant_r owl:someValuesFrom ?drug_sc .
  ?drug_participant_r rdf:type owl:Restriction .
  ?drug_participant_r owl:onProperty ?has_participant .
  ccp:RO_0000057 obo:IAO_0000219 ?has_participant . # RO_0000057 = RO:has_participant; IAO_0000219 = IAO:denotes
  filter (?has_participant != obo:RO_0000057) #RO_0000057 = RO:has_participant
  
  # Ee are looking for drug-target interactions, so here we ensure the 
  # has_participant restriction is an attribute of a 'binding' event.
  ?binding_event rdfs:subClassOf ?drug_participant_r .
  ?binding_event rdfs:subClassOf ?binding .
  ccp:GO_0005488 obo:IAO_0000219 ?binding . # GO_0005488 = GO:binding; IAO_0000219 = IAO:denotes
  filter (?binding != obo:GO_0005488) # GO_0005488 = GO:binding
  ?binding_ice obo:IAO_0000219 ?binding .
  
  # Now find other participants in the binding event (most likely target proteins).
  ?binding_event rdfs:subClassOf ?target_participant_r .
  ?target_participant_r rdf:type owl:Restriction .
  ?target_participant_r owl:onProperty ?has_participant .
  ?target_participant_r owl:someValuesFrom ?target_sc .
  ?target_sc rdfs:subClassOf ?target .
  ?target_ice obo:IAO_0000219 ?target . # return ICE identifier
  
  # From the target, find any genes that are templates for the drug targets
  # by following has_gene_template restrictions. Note that the has_gene_template
  # restrictions work on the canonical bioentities themselves (and not subclasses
  # of the canonical bioentities).
  ?target rdfs:subClassOf ?has_gene_template_r .
  ?has_gene_template_r rdf:type owl:Restriction .
  ?has_gene_template_r owl:onProperty ?has_gene_template .
  <http://ccp.ucdenver.edu/obo/ext/pr#has_gene_template> obo:IAO_0000219 ?has_gene_template . # IAO_0000219 = IAO:denotes
  filter(?has_gene_template != <http://purl.obolibrary.org/obo/pr#has_gene_template>)
  ?has_gene_template_r owl:someValuesFrom ?gene .
  ?gene_ice obo:IAO_0000219 ?gene .
  
  # Find things that the gene causes_or_contributes_to. Currently results will be 
  # limited to phenotypes b/c that is what is currently represented in kabob.
  ?gene_sc rdfs:subClassOf ?gene .
  ?gene_sc rdfs:subClassOf ?cause_r . 
  ?cause_r rdf:type owl:Restriction .
  ?cause_r owl:onProperty ?causes_or_contributes_to_condition .
  ccp:RO_0003302 obo:IAO_0000219 ?causes_or_contributes_to_condition . # RO_0003302 = RO:causes_or_contributes_to; IAO_0000219 = IAO:denotes
  filter (?causes_or_contributes_to_condition != obo:RO_0003302) # RO_0003302 = RO:causes_or_contributes_to
  ?cause_r owl:someValuesFrom ?phenotype_sc .
  ?phenotype_sc rdfs:subClassOf ?phenotype .
  ?phenotype_ice obo:IAO_0000219 ?phenotype .
  
  # collect labels to return
  ?drug rdfs:label ?drug_label .
  ?target rdfs:label ?target_label .
  ?phenotype rdfs:label ?phenotype_label .
  ?gene rdfs:label ?gene_label .
  
  }"

 
# Step 2 - Use SPARQL package to submit query and save results to a data frame
qd <- SPARQL(endpoint,query)
df <- qd$results

```
